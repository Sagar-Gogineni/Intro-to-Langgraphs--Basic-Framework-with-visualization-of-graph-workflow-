{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3744cda-6445-4b26-a65c-0693b2fd825e",
   "metadata": {},
   "source": [
    "Intro to Langgraph\n",
    "We build a simple research assistant where the llm will classifiy the given question as a research question or general question.\n",
    "Based on the classification llm routes the question to specific nodes to handle research quesiton and general quesiton. \n",
    "Install required libraries - langchian azureopenai langgraph grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67999135-eec8-4bcd-9f6a-f089b572c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph grandalf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97418e4-7f5d-4cfd-a03a-32213a359d41",
   "metadata": {},
   "source": [
    "Initialise your llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5705934e-3f9b-4d6e-b73e-cb937d063ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M319170\\AppData\\Local\\miniconda3\\envs\\new_test\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\M319170\\AppData\\Local\\miniconda3\\envs\\new_test\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://api.nlp.dev.uptimize.merckgroup.com to https://api.nlp.dev.uptimize.merckgroup.com/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\M319170\\AppData\\Local\\miniconda3\\envs\\new_test\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\M319170\\AppData\\Local\\miniconda3\\envs\\new_test\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://api.nlp.dev.uptimize.merckgroup.com to https://api.nlp.dev.uptimize.merckgroup.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.nlp.dev.uptimize.merckgroup.com\"#\"https://nlp.dev.uptimize.merckgroup.com\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"cbfe29d4-971a-4215-9030-6b674bb090d4\"#get_nlp_api_key() # never store your API key in your code\n",
    "\n",
    "# The extra headers won't be necessary in the future\n",
    "headers = {\n",
    "    \"x-api-key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "}\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4-32k-0613\",\n",
    "    model_name=\"gpt-4-32k-0613\",\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "#response_chat =llm([HumanMessage(content=\"write a cool poem about air and water\")])\n",
    "#print(response_chat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f35644-5074-4b60-a5b7-023b58be305f",
   "metadata": {},
   "source": [
    "Initialise Graph state with questionm classification and response - this helps in deciding the action to be taken by llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404354f1-9100-406a-bd7b-a215eccb0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict, Optional\n",
    "from langgraph.graph import StateGraph,END\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: Optional[str] = None\n",
    "    classification: Optional[str] = None\n",
    "    response: Optional[str] = None\n",
    "\n",
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07144dd5-d9cf-4291-a2a6-4e4919e6cb87",
   "metadata": {},
   "source": [
    "Define a function where llm classifies the question as Research question or an General question.This is basically our Supervisor LLM that decides what to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6780c5f-a616-4d07-b700-0254efa8f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(question):\n",
    "    result=llm.invoke(\"\"\"Classisfy the given question as an research question or an general question. If its a research question output it as 'Research' else 'General'.\n",
    "    DO NOT provide any commetns, or appolocgies. Your output shoud contain only a single word. Here is the quesiton\"\"\"+question)\n",
    "    return result.content\n",
    "#classify(\"where can i find India\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299b11d-7ba0-4736-babf-c225cca2febf",
   "metadata": {},
   "source": [
    "Function to handle classify question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c18717d-6d4f-49d8-ba64-5b1eacce3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_input_node(state):\n",
    "    question = state.get('question', '').strip()\n",
    "    classification = classify(question)  # Assume a function that classifies the input\n",
    "    return {\"classification\": classification}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614cf1ac-a23c-4d18-a127-694db1fd6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to handle research question\n",
    "#this is a basic function. You can add your own functions to handle different questions and inputs\n",
    "def handle_research_node(state):\n",
    "    return {\"response\": f\"Search result for '{question}'\"}\n",
    "\n",
    "\n",
    "#Function to handle general question\n",
    "def handle_search_node(state):\n",
    "    question = state.get('question', '').strip()\n",
    "    search_result = f\"Search result for '{question}'\"\n",
    "    return {\"response\": search_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb7782-3d72-481f-8185-bce9ce041b74",
   "metadata": {},
   "source": [
    "here we define nodes along with its respective functions. Nodes can be either function or any runnables such as Langchain LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f700b39b-e6db-4a01-8a05-2cae251ccd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"classify_input\", classify_input_node)\n",
    "workflow.add_node(\"handle_research\", handle_research_node)\n",
    "workflow.add_node(\"handle_search\", handle_search_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35d24d-38a0-4b9b-a341-18f1c5e2c9a6",
   "metadata": {},
   "source": [
    "here we use a decision function and create an conditional edge. this helps the supervisor to route the tasks to specifc nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf2d37a-abf8-4409-bd04-390ef93ece04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_next_node(state):\n",
    "    return \"handle_research\" if state.get('classification') == \"Research\" else \"handle_search\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_next_node,\n",
    "    {\n",
    "        \"handle_research\": \"handle_research\",\n",
    "        \"handle_search\": \"handle_search\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dec152-e4a7-4ed3-8802-67c6ca9ebfd5",
   "metadata": {},
   "source": [
    "Here we set an starting point of our graph using set entry point and add corresponding edges to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63672d44-c1e1-4396-9287-ac7b066a6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"classify_input\")\n",
    "workflow.add_edge('handle_research', END)\n",
    "workflow.add_edge('handle_search', END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa68f36-1c5f-4556-a4f5-c6da721fb48d",
   "metadata": {},
   "source": [
    "now we complie the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153199e8-605a-4051-ad75-719e60da9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ca5545f-36c5-491e-898f-f045d2474525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Hello, how are you?', 'classification': 'General', 'response': \"Search result for 'Hello, how are you?'\"}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Hello, how are you?\"}\n",
    "result = app.invoke(inputs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9138afc-345a-48b3-82fa-ddfb6c5d5935",
   "metadata": {},
   "source": [
    "below is the simple visualization for your graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26543dda-0c0a-49ba-ad2c-c497e69f8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  +-----------+                 \n",
      "                  | __start__ |                 \n",
      "                  +-----------+                 \n",
      "                        *                       \n",
      "                        *                       \n",
      "                        *                       \n",
      "               +----------------+               \n",
      "               | classify_input |               \n",
      "               +----------------+               \n",
      "                        *                       \n",
      "                        *                       \n",
      "                        *                       \n",
      "      +---------------------------------+       \n",
      "      | classify_input_decide_next_node |       \n",
      "      +---------------------------------+       \n",
      "                ***           ***               \n",
      "              **                 **             \n",
      "            **                     **           \n",
      "+-----------------+           +---------------+ \n",
      "| handle_research |           | handle_search | \n",
      "+-----------------+           +---------------+ \n",
      "                ***           ***               \n",
      "                   **       **                  \n",
      "                     **   **                    \n",
      "                  +---------+                   \n",
      "                  | __end__ |                   \n",
      "                  +---------+                   \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4409aab0-e408-4f4c-bc12-e67370912c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
